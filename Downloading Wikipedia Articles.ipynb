{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Downloading Wikipedia Articles\n",
    "\n",
    "In this notebook, we will download all of the latest wikipedia articles. After getting the data, we'll work on making sense of it using data science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "# import time\n",
    "# from keras.utils import get_file\n",
    "# try:\n",
    "#     from urllib.request import urlretrieve\n",
    "# except ImportError:\n",
    "#     from urllib import urlretrieve\n",
    "# import xml.sax\n",
    "\n",
    "# import subprocess\n",
    "# import mwparserfromhell\n",
    "# import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching through Wikipedia Dump\n",
    "\n",
    "To start, we make a request to the wikimedia dump of Wikipedia. We'll search through `enwiki` which has the English language dumps of wikipedia. This first request finds the available recent dumps and lists them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "\n",
    "# Find the links that are dates of dumps\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line of code finds the html of the dump for the first of September."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = base_url + '20180901/'\n",
    "\n",
    "# Retrieve the html\n",
    "dump_html = requests.get(dump_url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can parse through the html text to find the available files for download. We will focus only on the most recent revision of the articles themselves. It is possible to get the past history of articles, the edits, the discussion, and metadata, but the articles themselves provide us with more than enough data! \n",
    "\n",
    "For more information on the available downloads, take a look at the [Wikimedia dump](https://dumps.wikimedia.org/) or on [Wikipedia itself](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent revision of every single article (what we are after) is available in a single file as `pages-articles.xml.bz2`. However, we'll download the articles in smaller chunks so that we can then process it in parallel (rather than all sequentially). The single file - which is compressed XML (using bz) - is over 15 GB. \n",
    "\n",
    "To find all the individual files, we'll search through the html from the dump identified earlier. We'll look for any files that have `pages_articles` in the text. To parse through html (or other markup languages), we can use Beautiful Soup. If you are doing any web scraping, this will be a very useful library to learn! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "# Search through all files\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    # Select the relevant files\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "        \n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the partitioned files (each of which has tens or hundreds of thousands of articles), so we can further our selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = [file[0] for file in files if '.xml-p' in file[0]]\n",
    "files_to_download[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Now we need to actually download the data. This can be done using the keras `get_file` utility which downloads the specified file at the specified url. If we already have the entire dataset downloaded, then we don't want to download it again! For that reason we first use a check to see if the data exists.\n",
    "\n",
    "The default download directory for keras is `~/.keras/datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import get_file\n",
    "\n",
    "keras_home = '/home/ubuntu/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files_to_download:\n",
    "    path = keras_home + file\n",
    "    \n",
    "    # Check to see if the path exists\n",
    "    if not os.path.exists(keras_home + file):\n",
    "        print('Downloading')\n",
    "        # If not, download the file\n",
    "        data_paths.append(get_file(file, dump_url))\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # Otherwise extract information\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the largest file? We can use `sorted` to sort by the file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(file_info, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the file with the most articles? Just change the key for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(file_info, key = lambda x: x[2], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the largest files are about 400 MB compressed (bz2) and contain 150,000 articles. There should be a total of 58 million articles on English wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about putting this info into a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "file_df = pd.DataFrame(file_info, columns = ['file', 'size (MB)', 'articles']).set_index('file')\n",
    "file_df['size (MB)'].plot.bar(color = 'red', figsize = (12, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df['articles'].plot.bar(color = 'red', figsize = (12, 6));\n",
    "plt.ylabel('Number of Articles');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the upper limit for number of articles is 150,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {file_df['articles'].sum()} total articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 58,360,835 articles, which represents all of English language Wikipedia on September 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total size of files on disk is {file_df['size (MB)'].sum() / 1e3} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Through the Data\n",
    "\n",
    "Together, the files take up 15.4 GB. Decompressed as xml, it's close to 50 GB. We could decompress each file into XML and then parse through it, but we can also parse through the decompressed file one line at a time. If we are concered about disk space, this is a better option. Working through the compressed file one line at a time also might be the only option is cases where the individual files are too large to fit in memory. \n",
    "\n",
    "We'll start by working through one of the files and then develop functions that we can run on all of the files. Because we downloaded the data in chunks, we'll be able to parallelize the parsing operations using multiple threads on our machine.\n",
    "\n",
    "To get started, make sure you have `bzcat` installed on your system. The [`bzcat` utility] (http://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_utilities/b/bzcat.html) is a command line program that decompresses a bz2 compressed file and sends the contents to standard out. To go through the files one line at a time, we simply iterate over the command to decompress the file. We call the command using `subprocess` which is often used to execute system commands in Python.\n",
    "\n",
    "Another option for decompressing the file is `bz2`. However, in tests (see below), I found this to be much slower than `bzcat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import subprocess\n",
    "\n",
    "data_path = data_paths[15]\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "\n",
    "lines = []\n",
    "for i, line in enumerate(bz2.BZ2File(data_path, 'r')):\n",
    "    lines.append(line)\n",
    "    if i > 1e6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "\n",
    "lines = []\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    lines.append(line)\n",
    "    if i > 1e6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subprocess` + `bzcat` approach is nearly twice as fast. Let's run this again and see what kind of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    lines.append(line)\n",
    "    if i > 5e5:\n",
    "        break\n",
    "        \n",
    "lines[-165:-109]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a complete article. We could go through each article and extract out the information, but that would be extremely inefficient. Instead, we can use an xml parser to extract precisely the information we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Articles \n",
    "\n",
    "In order to get useful information from this data, we have to parse it on two levels.\n",
    "\n",
    "1. Extract the titles and article text from the XML\n",
    "2. Extract relevant information from the article text\n",
    "\n",
    "To solve the first problem, we'll use tool purpose built for the task of parsing XML, SAX: The Simple API for XML. [The documentation](http://pyxml.sourceforge.net/topics/howto/section-SAX.html) is a little difficult to follow, but the basic idea is that we can use SAX to search through the XML and select elements based on the tag. (If you need an introduction to XML, I'd highly recommend starting [here](https://www.w3schools.com/xml/default.asp)).\n",
    "\n",
    "For example, if we have the follow XML element, we want to extract the text that occurs between the `<title>` tags:\n",
    "\n",
    "`<title>Carroll Knicely</title>`\n",
    "\n",
    "Likewise, if we have the content of an article like below, we want to extract the text that occurs between the `<text>` tags. \n",
    "\n",
    "```XML\n",
    "<text xml:space=\"preserve\">\\'\\'\\'Carroll F. Knicely\\'\\'\\' (born c. 1929 in [[Staunton, Virginia]] - died November 2, 2006 in [[Glasgow, Kentucky]]) was [[Editing|editor]] and [[Publishing|publisher]] of the \\'\\'[[Glasgow Daily Times]]\\'\\' for nearly 20 years (and later, its owner) and served under three [[Governor of Kentucky|Kentucky Governors]] as commissioner and later Commerce Secretary.\\n'\n",
    "</text>\n",
    "```\n",
    "\n",
    "We'll use the SAX parser to do exactly that: find the titles and text content of the articles. Then, we can pass the text to another parser to extract information from the article. \n",
    "\n",
    "Explaining how SAX works is a little more difficult than just showing, so I'll present the code and show some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a `handler` object of the `WikiXMLHandler` class. Then we pass the handler in as the content handler to a SAX `parser`. Basically, we are overriding the default SAX parser in order to do what we want: find the titles and texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "handler._pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a single article. We `feed` in one line of XML at a time to the `parser`. It searches the XML for the tags using the methods in the handler. The correct data is then stored in the handler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lines[-165:-109]:\n",
    "    parser.feed(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully extracted one page! Once we have the page text, we need to process it as well to find the information we want. We'll write that function next. First, let's see this process again, this time finding two different articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 3 articles have been found\n",
    "    if len(handler._pages) > 2:\n",
    "        break\n",
    "        \n",
    "print([x[0] for x in handler._pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to process an entire article. For this, we will turn to the `mwparserfromhell` library. [This library](https://github.com/earwig/mwparserfromhell) is custom made for parsing `MediaWiki` wikicode. This includes Wikipedia articles. [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki) is used by Wikipedia and numerous other projects and provides a relatively standardized template for creating wiki pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Articles\n",
    "\n",
    "The best way to figure out how to parse an article is simply to do it! Let's work through the article for `Zhou Wei Hui`. All we need to do is pass the Wikipedia article text to the `mwparserfromhell`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 3 articles have been found\n",
    "    if len(handler._pages) > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell \n",
    "\n",
    "print(handler._pages[6][0])\n",
    "wiki = mwparserfromhell.parse(handler._pages[6][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a normal string, but in fact, it's a `mwparserfromhell.wikicode.Wikicode` object with many different methods for sorting through the content. For example, we can find all the internal links (those that go to other wikipedia pages) using `wiki.filter_wikilinks()`. This will give us the `title` of the Wikipedia article linked to as well as the `text` of the link. We'll extract just the titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilinks = [x.title for x in wiki.filter_wikilinks()]\n",
    "print(f'There are {len(wikilinks)} wikilinks.')\n",
    "wikilinks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't get the discussion or the edits around the articles, so we won't be able to find this information. However, if you do decide to grab comments, edits, revisions, etc., you can use `mwparserfromhell` to extract all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.filter_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.filter_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out everything you can do with `mwparserfromhell`, [read the docs](https://mwparserfromhell.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the external links (those that go outside of Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links = [(x.title, x.url) for x in wiki.filter_external_links()]\n",
    "print(f'There are {len(external_links)} external links.')\n",
    "external_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search through the text for specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contemporary = wiki.filter(matches = 'contemporary')\n",
    "contemporary[1], type(contemporary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contemporary[0], type(contemporary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Templates\n",
    "\n",
    "The easiest way to filter articles to a category (at least that I've found) is through the use of templates. These are boxes of information that are standardized across a category. For example, the template of the `KENZ (FM)` radio station looks like this:\n",
    "\n",
    "![Radio Station Infobox](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/images/radio_template.PNG?raw=true)\n",
    "\n",
    "These are called `Infobox` and there are many of them, each one for a different category such as films or books. Let's take a look at the templates for this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = wiki.filter_templates()\n",
    "print(f'There are {len(templates)} templates.')\n",
    "for template in templates:\n",
    "    print(template.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different templates. In this case, the Infobox is name `Infobox radio station`. If we wanted to find all of the radio station articles, then the easiest way would be to search every article for this template. We can search for a specific template as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = wiki.filter_templates(matches = 'Infobox radio station')[0]\n",
    "infobox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes of the infobox can be accessed and put into a dictionary using the `name` and the `value`. To clean things up, we first strip the code and then strip whitespace and escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = {param.name.strip_code().strip(): param.value.strip_code().strip() for param in infobox.params}\n",
    "information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a pretty clean look at the summary information for this article. In this project, we aren't going to be working with the text of the articles - that's an entirely separate undertaking - but we can use information such as links to build a book recommendation engine. \n",
    "\n",
    "### Searching for Books\n",
    "\n",
    "Now that we have an understanding of how to parse an article, we can start searching for what we want: all the books on Wikipedia! The books can be identified because they use an Infobox book template.\n",
    "\n",
    "![Infobox book](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/images/book_template.PNG?raw=true)\n",
    "\n",
    "We simply have to filter the article for the `Infobox book` template, and if it's present, store the information. If not, then we move to the next article. The function below is designed to find can return book articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_article(title, text):\n",
    "    \"\"\"Process a wikipedia article looking for books\"\"\"\n",
    "    \n",
    "    # Create a parsing object\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    \n",
    "    # Search through templates for the book template\n",
    "    book = wikicode.filter_templates(matches = 'Infobox book')\n",
    "    \n",
    "    if len(book) >= 1:\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in book[0].params\n",
    "                      if param.value.strip_code().strip()}\n",
    "        wikilinks = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "        exlinks = [x.url.strip_code().strip() for x in wikicode.filter_external_links()]\n",
    "        return (title, properties, wikilinks, exlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_article('KENZ (FM)', wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No such luck with this article. We'll have to search through the entire compressed XML file to find some books. To pass the text from each article, we slightly have to modify the `Handler` class. This time, in the `endElement` function, if the article ends (the tags is `page`) then we send the title and the contents (`text`) to the `process_article` function. \n",
    "\n",
    "This function will return either nothing if it doesn't find a book, or the book properties, Wikilinks, and external links if it does find a book. These will be added as a list to the `handler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._books = []\n",
    "        self._article_count = 0\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._article_count += 1\n",
    "            # Search through the page to see if a book\n",
    "            book = process_article(**self._values)\n",
    "            if book:\n",
    "                self._books.append(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below stops when we've found 2 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched through 1974 articles to find 2 books.\n"
     ]
    }
   ],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 3 articles have been found\n",
    "    if len(handler._books) > 2:\n",
    "        break\n",
    "        \n",
    "print(f'Searched through {handler._article_count} articles to find 2 books.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Cambridge Dictionary of Philosophy',\n",
       " {'1': '< !-- See Wikipedia:WikiProject_Books -- >',\n",
       "  'name': 'The Cambridge Dictionary of Philosophy',\n",
       "  'image': 'File:The Cambridge Dictionary of Philosophy (first edition).jpg',\n",
       "  'caption': 'Cover of the first edition',\n",
       "  'editor': 'Robert Audi',\n",
       "  'country': 'United Kingdom',\n",
       "  'language': 'English',\n",
       "  'subject': 'Philosophy',\n",
       "  'publisher': 'Cambridge University Press',\n",
       "  'pub_date': '1995',\n",
       "  'media_type': 'Print (Hardcover and Paperback)',\n",
       "  'pages': '1001 (second edition)',\n",
       "  'isbn': '0-521-63722-8',\n",
       "  'isbn_note': '(second edition)'},\n",
       " ['Robert Audi',\n",
       "  'Philosophy',\n",
       "  'Cambridge University Press',\n",
       "  'Hardcover',\n",
       "  'Paperback',\n",
       "  'dictionary',\n",
       "  'philosophy',\n",
       "  'Cambridge University Press',\n",
       "  'Robert Audi',\n",
       "  'Category:1995 books',\n",
       "  'Category:Cambridge University Press books',\n",
       "  'Category:Dictionaries of philosophy',\n",
       "  'Category:Encyclopedias of philosophy',\n",
       "  'Category:English-language books'],\n",
       " ['http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=9780521637220'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler._books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each book, we have the title, the information contained in the `Infobox`, the internal Wikilinks, and the external links. Using just this information, we'll be able to build a fairly robust book recommendation system! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long it would take to search through just one partition. Uncompressed, the size of this partition is 1.5 GB with over 24 million lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/ubuntu/.keras/datasets/p15.xml'):\n",
    "    subprocess.call(['bzcat /home/ubuntu/.keras/datasets/enwiki-20180901-pages-articles15.xml-p7744803p9244803.bz2 >> p15.xml'],\n",
    "                    shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24097749  164409183 1538502087 /home/ubuntu/.keras/datasets/p15.xml\r\n"
     ]
    }
   ],
   "source": [
    "!wc /home/ubuntu/.keras/datasets/p15.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the entire file\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f'Processed {i + 1} lines so far.', end = '\\r')\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "end = timer()\n",
    "books = handler._books\n",
    "\n",
    "print(f'\\n Searched through {handler._article_count} articles.')\n",
    "print(f'\\nFound {len(books)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('p15_books.ndjson', 'wt') as fout:\n",
    "    for l in books:\n",
    "        fout.write(json.dumps(l) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_in = []\n",
    "with open('p15_books.ndjson', 'rt') as fin:\n",
    "    for l in fin.readlines():\n",
    "        book_in.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_in[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Through all Files\n",
    "\n",
    "The next step is to search through all of the files. We downloaded them in pieces precisely so we could parallelize this operation and speed things up! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to write a function that takes in a file and returns a list of the book articles. We already have all the parts defined so we can put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_books(data_path):\n",
    "    # Object for handling xml\n",
    "    handler = WikiXmlHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data_path), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return handler._books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,\n",
       " '/home/ubuntu/.keras/datasets/enwiki-20180901-pages-articles17.xml-p11539268p13039268.bz2')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions = [keras_home + file for file in os.listdir(keras_home) if 'xml-p' in file]\n",
    "len(partitions), partitions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Code Using MutliThreading\n",
    "\n",
    "The next block of codes using multithreading to progress many files at once. Because these tasks are input / output bound, and not cpu bound, we can use threads to carry out multiple tasks at the same time. Learning multithreading / multiprocessing is essential for making your data science workflows more efficient. I'd recommend [this article](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b) to get started with the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfac411e079041edbf284efb1dd545b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool \n",
    "import tqdm \n",
    "\n",
    "start = timer()\n",
    "pool = Pool(processes = 4)\n",
    "results = []\n",
    "\n",
    "for x in tqdm.tqdm_notebook(pool.imap_unordered(find_books, partitions), total = len(partitions)):\n",
    "    results.append(x)\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 605 books in 769 seconds.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "book_list = list(chain(*results))\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee80e618e5d4290b1ac8ce9ee724eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "start = timer()\n",
    "pool = ThreadPool(processes = 20)\n",
    "results = []\n",
    "\n",
    "for x in tqdm.tqdm_notebook(pool.imap_unordered(find_books, partitions), total = len(partitions)):\n",
    "    results.append(x)\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 605 books in 1823 seconds.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "book_list = list(chain(*results))\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdb2f4a7a01489799ca16743eb37fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Pool \n",
    "import tqdm \n",
    "\n",
    "start = timer()\n",
    "pool = Pool(processes = 4)\n",
    "results = []\n",
    "\n",
    "for x in tqdm.tqdm_notebook(pool.imap_unordered(find_books, partitions), total = len(partitions)):\n",
    "    results.append(x)\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "book_list = list(chain(*results))\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_books.ndjson', 'wt') as fout:\n",
    "    for book in book_list:\n",
    "         fout.write(json.dumps(book) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to download the most recent version of every article on English language Wikipedia. Not only are we able to download all of the information, but we also saw some tools for processing this information to extract the data that we need. Wikipedia is an incredible resource, not only for doing your school projects, but also for exploring techniques in data science. In future notebooks, we'll look at how to build a book recommendation engine based on the data we've collected here. Gathering the data is one thing, but eventually, we want to be able to accomplish useful tasks with this data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the  Effectively what this code is doing is decompressing the file one line at a time and sending the line through the `parser`. This gets around the need to load the entire file into memory at once since it is probably too large in its uncompressed state. \n",
    "\n",
    "The first time, we set the code to break if the handler encouters any books so we can look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                                         stdin = open(data_path), stdout = subprocess.PIPE).stdout):\n",
    "    x = line\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    if handler._books:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry is simply the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._books[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is all the parameters that are in the `infobox book` template on the wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._books[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry is all of the Wikipedia links that occur on the page. These are links that go to __other wikipedia pages__ as opposed to external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._books[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll gather this information for every article on Wikipedia that has an `infobox book` template on the page (this should be around 40,000). While this won't capture every book, it will give us a large selection to work with for making recommendations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = process_article(**handler._values, return_wikicode=True)\n",
    "processed.filter_templates()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Every Book on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "start = timer()\n",
    "recorded_count = 0\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                                         stdin = open(data_path), stdout = subprocess.PIPE).stdout):\n",
    "    # Process the line (entry)\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "    # Print progress information\n",
    "    n_books = len(handler._books)\n",
    "    if (n_books % 1000 == 0) and (n_books != recorded_count):\n",
    "        print(f'{n_books} books found. {round(timer() - start)} seconds elapsed.', end = '\\r')\n",
    "        # Make sure to only report found books once\n",
    "        recorded_count = n_books\n",
    "        \n",
    "        with open(f'generated/{n_books}-books.ndjson', 'wt') as fout:\n",
    "            for book in handler._books:\n",
    "                 fout.write(json.dumps(book) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated/all_books.ndjson', 'wt') as fout:\n",
    "    for book in handler._books:\n",
    "         fout.write(json.dumps(book) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
