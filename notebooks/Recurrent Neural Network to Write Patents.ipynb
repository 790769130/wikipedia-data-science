{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Writing Patents Using a Recurrent Neural Network\n",
    "\n",
    "The purpose of this notebook is to develop a recurrent neural network which can be used to write patent abstracts. Although this is mostly meant as a simple example, the idea of recurrent neural networks is powerful and can be usde for real purposes such as generating text similar to a corpus, machine translation, and supervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "CHUNK_SIZE = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6382"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in os.listdir('../data/patents_parsed/'):\n",
    "    with open(f'../data/patents_parsed/{file}', 'rt') as fin:\n",
    "        data.append([json.loads(l) for l in fin])\n",
    "        \n",
    "        \n",
    "data = list(chain(*data))\n",
    "data = [r for r in data if r[0] is not None]\n",
    "data = [r for r in data if len(r[0]) >= 200]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(x[0]) for x in data]\n",
    "min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Artificial intelligence system for item analysis for rework shop orders ',\n",
       " 'A computer implemented method facilitates the capability for shop re-work orders to be effectively scheduled, knowing the time and location of item availability that is needed to correct the problem found in the re-work shop orders. The system automatically identifies alternate components or items that can be used in the shop orders and provides realistic shipping dates so that the re-work shop orders can be scheduled. If components or items are not available, the system provides feedback to the material planning system to re-plan items using traditional material planning systems such as the MRP (material requirement planning) systems and provide projected shipping dates so that re-work orders can be scheduled.')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1], data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = [d[0] for d in data]\n",
    "titles = [d[1] for d in data]\n",
    "\n",
    "chars = []\n",
    "for abstract in abstracts:\n",
    "    for ch in abstract:\n",
    "        chars.append(ch)\n",
    "        \n",
    "chars = set(chars)\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 'A')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "char_to_idx['a'], idx_to_char[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_rnn_model(num_chars, num_layers, num_nodes = 512, dropout = 0.1):\n",
    "    # Take in a sequence of one-hot encoded characters\n",
    "    input_layer = Input(shape = (None, num_chars), name = 'input')\n",
    "    prev = input_layer\n",
    "    \n",
    "    # Add an LSTM cell for each layer\n",
    "    for i in range(num_layers):\n",
    "        lstm = LSTM(num_nodes, return_sequences = True, name = f'lstm_layer_{i}')(prev)\n",
    "        if dropout:\n",
    "            prev = Dropout(dropout)(lstm)\n",
    "        else:\n",
    "            prev = lstm\n",
    "            \n",
    "    # For each time step find the most likely character - one time step considers up to current character\n",
    "    # Time Distributed applies same layer to all time steps (first dimension)\n",
    "    dense = TimeDistributed(Dense(num_chars, name = 'dense',\n",
    "                             activation = 'softmax'))(prev)\n",
    "    model = Model(inputs = [input_layer], outputs = [dense])\n",
    "    \n",
    "    # Compile with categorical loss\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer = RMSprop(lr=0.01), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None, 147)         0         \n",
      "_________________________________________________________________\n",
      "lstm_layer_0 (LSTM)          (None, None, 512)         1351680   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, None, 147)         75411     \n",
      "=================================================================\n",
      "Total params: 1,427,091\n",
      "Trainable params: 1,427,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = char_rnn_model(len(chars), 1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The present invention describes the use of autonomous devices, which can be arranged in networks, such as neural networks, to better identify, track, and acquire sources of signals present in an environment. The environment may be a physical environment, such as a battlefield, or a more abstract environment, such as a communication network. The devices may be mobile, in the form of vehicles with sensors, or may be information agents, and may also interact with one another, thus allowing for a great deal of flexibility in carrying out a task. In some cases, the devices may be in the form of autonomous vehicles which can collaboratively sense, identify, or classify a number of sources or targets concurrently. The autonomous devices may function as mobile agents or attractors in a network, such as a neural network. The devices may also be aggregated to form a network of networks and provide scalability to a system in which the autonomous devices are operating.']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(abstracts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_generator(text, char_to_idx, batch_size, chunk_size):\n",
    "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    \n",
    "    chunk_size_original = chunk_size\n",
    "    \n",
    "    # Generator yields samples\n",
    "    while True:\n",
    "        # Batch size is number of samples to use\n",
    "        for row in range(batch_size):\n",
    "            \n",
    "            # Choose a random abstract\n",
    "            sample = random.sample(text, 1)[0]\n",
    "            \n",
    "            # Choose a random starting index\n",
    "            idx = random.randrange(len(sample) - chunk_size - 1)\n",
    "\n",
    "            # Empty array to hold a chunk, chunk size is number of characters to extract\n",
    "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
    "            \n",
    "            # Need to find one more than chunk size to make labels\n",
    "            for i in range(chunk_size + 1):\n",
    "                chunk[i, char_to_idx[sample[idx + i]]] = 1\n",
    "                \n",
    "            # Features are all characters except for last\n",
    "            X[row, :, :] = chunk[:chunk_size]\n",
    "            # Labels are all characters except for first\n",
    "            y[row, :, :] = chunk[1:]\n",
    "            \n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys = next(data_generator(abstracts, char_to_idx, 512, chunk_size = 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 80, 147)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 147)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = Xs[1]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volutional layers of a trained convolution neural network for determining a conv'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "\n",
    "for row in sample:\n",
    "    x.append(idx_to_char[np.argmax(row)])\n",
    "''.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volutional layers of a trained convolution neural network for determining a conv'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "\n",
    "for row in sample:\n",
    "    y.append(idx_to_char[np.argmax(row)])\n",
    "''.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is a shifted forward version of the features. At each feature, we are teaching the network to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor = 'loss', min_delta = 0.03, patience = 5),\n",
    "             ModelCheckpoint(filepath = '../models/first_rnn.h5', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5332623"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "all_text = list(chain(*abstracts))\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      " - 68s - loss: 2.8362 - acc: 0.2256\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:432: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 67s - loss: 1.9862 - acc: 0.4509\n",
      "Epoch 3/40\n",
      " - 67s - loss: 1.6189 - acc: 0.5733\n",
      "Epoch 4/40\n",
      " - 67s - loss: 1.4580 - acc: 0.6143\n",
      "Epoch 5/40\n",
      " - 67s - loss: 1.4041 - acc: 0.6314\n",
      "Epoch 6/40\n",
      " - 67s - loss: 1.4180 - acc: 0.6343\n",
      "Epoch 7/40\n",
      " - 67s - loss: 1.3666 - acc: 0.6463\n",
      "Epoch 8/40\n",
      " - 67s - loss: 1.4145 - acc: 0.6402\n",
      "Epoch 9/40\n",
      " - 67s - loss: 1.3861 - acc: 0.6458\n",
      "Epoch 10/40\n",
      " - 67s - loss: 1.3029 - acc: 0.6620\n",
      "Epoch 11/40\n"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(abstracts, char_to_idx, 256, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "h = model.fit_generator(generator=train_gen, epochs = 40, callbacks = callbacks,\n",
    "                        steps_per_epoch = 2 * len(all_text) / (BATCH_SIZE * CHUNK_SIZE),\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def generate_output(model, text, start_index = 2, diversity = None, amount = 400):\n",
    "    \n",
    "    if start_index is None:\n",
    "        start_index = random.randint(0, CHUNK_SIZE)\n",
    "        \n",
    "    sample = random.sample(text, 1)[0]\n",
    "    generated = sample[start_index: start_index + CHUNK_SIZE]\n",
    "    yield generated + '#'\n",
    "    \n",
    "    for i in range(amount):\n",
    "        x = np.zeros((1, len(generated), len(chars)))\n",
    "        for t, char in enumerate(generated):\n",
    "            x[0, t, char_to_idx[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x, verbose = 0)[0]\n",
    "    \n",
    "        if diversity is None:\n",
    "            next_index = np.argmax(preds[len(generated) - 1])\n",
    "            \n",
    "        else:\n",
    "            preds = np.array(preds[len(generated) - 1]).astype(np.float64)\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds, 1)\n",
    "            next_index = np.argmax(preds)\n",
    "            \n",
    "        next_char = idx_to_char[next_index]\n",
    "        yield next_char\n",
    "        \n",
    "        generated += next_char\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in generate_output(model, abstracts):\n",
    "    sys.stdout.write(ch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in generate_output(model, abstracts):\n",
    "    sys.stdout.write(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we got a small glimpse at the abilities of recurrent neural networks. Using just a few thousands patents and a basic neural network, we were able to teach a machine to produce reasonable outputs of patent abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
