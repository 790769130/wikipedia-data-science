{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:44:22.792507Z",
     "start_time": "2018-10-15T00:44:22.596333Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import utils\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:44:27.165006Z",
     "start_time": "2018-10-15T00:44:26.922953Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in os.listdir('../data/patents_parsed/'):\n",
    "    with open(f'../data/patents_parsed/{file}', 'rt') as fin:\n",
    "        data.append([json.loads(l) for l in fin])\n",
    "\n",
    "data = list(chain(*data))\n",
    "data = [r for r in data if r[0] is not None]\n",
    "data = [r for r in data if len(r[0]) >= 200]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find set of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:44:50.664053Z",
     "start_time": "2018-10-15T00:44:48.171380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = [d[0] for d in data]\n",
    "titles = [d[1] for d in data]\n",
    "\n",
    "chars = []\n",
    "for abstract in abstracts:\n",
    "    for ch in abstract:\n",
    "        chars.append(ch)\n",
    "\n",
    "chars = set(chars)\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:49:30.122765Z",
     "start_time": "2018-10-15T00:49:29.492266Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, filters='!\"#$%&(),:;.?*+-/@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:49:37.731560Z",
     "start_time": "2018-10-15T00:49:37.715974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16114,\n",
       " [('the', 63232),\n",
       "  ('a', 41963),\n",
       "  ('of', 32333),\n",
       "  ('and', 24302),\n",
       "  ('to', 22307),\n",
       "  ('for', 12682),\n",
       "  ('in', 12644),\n",
       "  ('is', 11846),\n",
       "  ('an', 9603),\n",
       "  ('data', 8254)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = tokenizer.word_counts\n",
    "wcs = sorted(wc.items(), key = lambda x: x[1], reverse = True)\n",
    "len(wc), wcs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:50:42.873263Z",
     "start_time": "2018-10-15T00:50:42.435041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.texts_to_sequences(abstracts)\n",
    "len(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pad_sequences(tokens, padding = 'post')\n",
    "len(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T00:51:53.729230Z",
     "start_time": "2018-10-15T00:51:53.724344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a system is provided to reduce noise from a signal of speech that is contaminated by noise the present system employs an artificial intelligence that is capable of deciding upon the adjustment of a filter subsystem by distinguishing between noise and speech in the spectrum of the incoming signal of speech plus noise the system does this by testing the pattern of a power or envelope function of the frequency spectrum of the incoming signal the system determines that the fast changing portions of that envelope denote speech whereas the residual is determined to be the frequency distribution of the noise power this determination is done while examining either the whole spectrum or frequency bands thereof regardless of where the maximum of the spectrum lies in another embodiment of the invention a feedback loop is incorporated which provides incremental adjustments to the filter by employing a gradient search procedure to attempt to increase certain speech like features in the system's output the present system does not require consideration of minima of functions of the incoming signal or pauses in speech instead the present system employs an artificial intelligence system to which is input the envelope pattern of the incoming signal of speech and noise the present system then filters out of this envelope signal the rapidly changing variations of the envelope over fixed time windows\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back = []\n",
    "\n",
    "for i in sequences[1]:\n",
    "    back.append(tokenizer.index_word.get(i))\n",
    "' '.join([x for x in back if x is not None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T01:37:59.923485Z",
     "start_time": "2018-10-15T01:37:59.918565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6382"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T01:28:18.670202Z",
     "start_time": "2018-10-15T01:28:18.667275Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "import gensim\n",
    "from subprocess import call\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.zip'\n",
    "\n",
    "if not os.path.exists(glove_vectors):\n",
    "    glove_vectors = get_file('glove.6B.zip', 'http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(f'unzip {glove_vectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 101)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.100d.txt'\n",
    "# w2v_model = gensim.models.KeyedVectors.load_word2vec_format(glove_vectors, binary = True)\n",
    "\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = glove[:, 0]\n",
    "vectors = glove[:, 1:].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {word: vector for word, vector in zip(words, vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16115, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, vectors.shape[1]))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2178"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_count = 0\n",
    "for idx, word in tokenizer.index_word.items():\n",
    "    vector = word_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[idx, :] = vector\n",
    "    else:\n",
    "        not_in_count += 1\n",
    "        \n",
    "not_in_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   10,    29,    22,    18,    11,    42,   785,   415,    20,\n",
       "         7480,     5,   434,     1,   161,    24,   313,    24,  1102,\n",
       "         1177,    10,   113,    22,     2,    46,     3,    10,  6839,\n",
       "         2436,    25,    11,  1260,  1102,  2592,   453,     6,  1241,\n",
       "         3454,   515,   249,   140,  1071,     2,   360,   109,  1407,\n",
       "            2,    46,     3,  2866,    11,  3605,  2705,   182,    13,\n",
       "           24,   869,     6,    20,   249,   398,     6,  1241,   515,\n",
       "          165,   249,   140, 13273,   294,   164,     6,    26,  8742,\n",
       "          301,    20,   706,     9,  3454,   147,   515,   165,     3,\n",
       "          448,     1,   360,   109,  2984,  1407,     1,    46,     3,\n",
       "         3620,   182,    62,     6,    20, 10397,   301,    37,     1,\n",
       "          228]), array([[   29],\n",
       "        [   22],\n",
       "        [   18],\n",
       "        [   11],\n",
       "        [   42],\n",
       "        [  785],\n",
       "        [  415],\n",
       "        [   20],\n",
       "        [ 7480],\n",
       "        [    5],\n",
       "        [  434],\n",
       "        [    1],\n",
       "        [  161],\n",
       "        [   24],\n",
       "        [  313],\n",
       "        [   24],\n",
       "        [ 1102],\n",
       "        [ 1177],\n",
       "        [   10],\n",
       "        [  113],\n",
       "        [   22],\n",
       "        [    2],\n",
       "        [   46],\n",
       "        [    3],\n",
       "        [   10],\n",
       "        [ 6839],\n",
       "        [ 2436],\n",
       "        [   25],\n",
       "        [   11],\n",
       "        [ 1260],\n",
       "        [ 1102],\n",
       "        [ 2592],\n",
       "        [  453],\n",
       "        [    6],\n",
       "        [ 1241],\n",
       "        [ 3454],\n",
       "        [  515],\n",
       "        [  249],\n",
       "        [  140],\n",
       "        [ 1071],\n",
       "        [    2],\n",
       "        [  360],\n",
       "        [  109],\n",
       "        [ 1407],\n",
       "        [    2],\n",
       "        [   46],\n",
       "        [    3],\n",
       "        [ 2866],\n",
       "        [   11],\n",
       "        [ 3605],\n",
       "        [ 2705],\n",
       "        [  182],\n",
       "        [   13],\n",
       "        [   24],\n",
       "        [  869],\n",
       "        [    6],\n",
       "        [   20],\n",
       "        [  249],\n",
       "        [  398],\n",
       "        [    6],\n",
       "        [ 1241],\n",
       "        [  515],\n",
       "        [  165],\n",
       "        [  249],\n",
       "        [  140],\n",
       "        [13273],\n",
       "        [  294],\n",
       "        [  164],\n",
       "        [    6],\n",
       "        [   26],\n",
       "        [ 8742],\n",
       "        [  301],\n",
       "        [   20],\n",
       "        [  706],\n",
       "        [    9],\n",
       "        [ 3454],\n",
       "        [  147],\n",
       "        [  515],\n",
       "        [  165],\n",
       "        [    3],\n",
       "        [  448],\n",
       "        [    1],\n",
       "        [  360],\n",
       "        [  109],\n",
       "        [ 2984],\n",
       "        [ 1407],\n",
       "        [    1],\n",
       "        [   46],\n",
       "        [    3],\n",
       "        [ 3620],\n",
       "        [  182],\n",
       "        [   62],\n",
       "        [    6],\n",
       "        [   20],\n",
       "        [10397],\n",
       "        [  301],\n",
       "        [   37],\n",
       "        [    1],\n",
       "        [  228],\n",
       "        [   48]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(sequences, batch_size, num_words, start_index = None):\n",
    "    X = np.zeros((batch_size, num_words), dtype=int)\n",
    "    y = np.zeros((batch_size, num_words), dtype = int)\n",
    "    \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            text = random.choice(sequences)\n",
    "            max_index = np.max(np.where(text != 0)) - num_words - 1\n",
    "            start_index = random.randint(0, max(0, max_index))\n",
    "            chunk = text[start_index: start_index + num_words + 1]\n",
    "            X[i, :] = np.array(chunk[:num_words]).astype(int)\n",
    "            y[i, :] = np.array(chunk[1:]).astype(int)\n",
    "        yield X, np.expand_dims(y, 2)\n",
    "            \n",
    "            \n",
    "xs, ys = next(data_generator(sequences, 4, 100))\n",
    "xs[0], ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1611500   \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         117248    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 100)         12900     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 16115)       1627615   \n",
      "=================================================================\n",
      "Total params: 3,369,263\n",
      "Trainable params: 1,757,763\n",
      "Non-trainable params: 1,611,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_words, output_dim = embedding_matrix.shape[1], \n",
    "                    weights = [embedding_matrix], mask_zero = True, trainable = False))\n",
    "model.add(Masking(mask_value = 0.0))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "model.add(TimeDistributed(Dense(num_words, activation = 'softmax')))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 120s 1s/step - loss: 7.0401 - acc: 0.0716\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 119s 1s/step - loss: 6.4404 - acc: 0.0769\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 119s 1s/step - loss: 6.2552 - acc: 0.0894\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 119s 1s/step - loss: 6.0271 - acc: 0.1116\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 119s 1s/step - loss: 5.8745 - acc: 0.1281\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 119s 1s/step - loss: 5.7477 - acc: 0.1406\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 120s 1s/step - loss: 5.6579 - acc: 0.1454\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 120s 1s/step - loss: 5.5752 - acc: 0.1527\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 120s 1s/step - loss: 5.4909 - acc: 0.1576\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 120s 1s/step - loss: 5.4142 - acc: 0.1618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4400090828>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = data_generator(sequences, 128, 200)\n",
    "model.fit_generator(train_gen, steps_per_epoch= 2 * len(sequences) // 128, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.444px",
    "left": "1627.56px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
